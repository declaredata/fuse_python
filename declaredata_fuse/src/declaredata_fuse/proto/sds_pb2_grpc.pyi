"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""

import abc
import collections.abc
import grpc
import grpc.aio
from declaredata_fuse.proto import sds_pb2
import typing

_T = typing.TypeVar("_T")

class _MaybeAsyncIterator(
    collections.abc.AsyncIterator[_T],
    collections.abc.Iterator[_T],
    metaclass=abc.ABCMeta,
): ...
class _ServicerContext(grpc.ServicerContext, grpc.aio.ServicerContext):  # type: ignore[misc, type-arg]
    ...

class sdsStub:
    """//////////////////////////////////////
    session/dataframe initialization methods
    //////////////////////////////////////
    """

    def __init__(
        self, channel: typing.Union[grpc.Channel, grpc.aio.Channel]
    ) -> None: ...
    ExecuteSql: grpc.UnaryUnaryMultiCallable[
        sds_pb2.ExecuteSqlRequest,
        sds_pb2.DataFrameUID,
    ]
    """execute SQL against a session and return a new DataFrame representing
    the result
    """

    CreateSession: grpc.UnaryUnaryMultiCallable[
        sds_pb2.Empty,
        sds_pb2.SessionUID,
    ]
    """create a new session with no DataFrames therein, then return its UID"""

    LoadCSV: grpc.UnaryUnaryMultiCallable[
        sds_pb2.LoadFileRequest,
        sds_pb2.DataFrameUID,
    ]
    """Load a CSV into a DataFrame, then return its UID"""

    LoadParquet: grpc.UnaryUnaryMultiCallable[
        sds_pb2.LoadFileRequest,
        sds_pb2.DataFrameUID,
    ]
    """Load a Parquet file into a DataFrame, then return its UID"""

    LoadJSON: grpc.UnaryUnaryMultiCallable[
        sds_pb2.LoadFileRequest,
        sds_pb2.DataFrameUID,
    ]
    """Load a JSON file into a DataFrame, then return its UID"""

    CloseSession: grpc.UnaryUnaryMultiCallable[
        sds_pb2.SessionUID,
        sds_pb2.Empty,
    ]
    """//////////////////////////////////////
    session destruction methods
    //////////////////////////////////////

    Close a session and free all associated dataframes.

    This is a highly destructive method, because all dataframes
    created directly or indirectly as part of this session
    will be instantly deleted.
    """

    SaveDataFrameAsTable: grpc.UnaryUnaryMultiCallable[
        sds_pb2.SaveDataFrameAsTableRequest,
        sds_pb2.Empty,
    ]
    """//////////////////////////////////////
    dataframe instance methods
    //////////////////////////////////////

    save a dataframe as a table, so that you can execute SQL queries 
    against it
    """

    PrettyPrintDataframe: grpc.UnaryUnaryMultiCallable[
        sds_pb2.DataFrameUID,
        sds_pb2.PrettyPrintDataframeResponse,
    ]
    """pretty-print a given dataframe.

    TODO: throw an error if the dataframe is too big to pretty-print
    """

    LimitDataFrame: grpc.UnaryUnaryMultiCallable[
        sds_pb2.LimitDataFrameRequest,
        sds_pb2.DataFrameUID,
    ]
    """filter an existing DataFrame, and return a new DataFrame"""

    SortDataFrame: grpc.UnaryUnaryMultiCallable[
        sds_pb2.SortDataFrameRequest,
        sds_pb2.DataFrameUID,
    ]
    """sort a dataframe by 1 or more column(s)"""

    FilterDataFrame: grpc.UnaryUnaryMultiCallable[
        sds_pb2.FilterDataFrameRequest,
        sds_pb2.DataFrameUID,
    ]
    """filter a dataframe according to 1 or more filter conditions"""

    Aggregate: grpc.UnaryUnaryMultiCallable[
        sds_pb2.AggregateRequest,
        sds_pb2.DataFrameUID,
    ]
    """group by, then aggregate a dataframe's data, the return a new dataframe"""

    WithColumn: grpc.UnaryUnaryMultiCallable[
        sds_pb2.WithColumnRequest,
        sds_pb2.DataFrameUID,
    ]
    """add a new column -- optionally by doing some calculation -- to a 
    given dataframe
    """

    Select: grpc.UnaryUnaryMultiCallable[
        sds_pb2.SelectRequest,
        sds_pb2.DataFrameUID,
    ]
    """project a DataFrame onto a new one, optionally by calculating new
    values
    """

    Collect: grpc.UnaryUnaryMultiCallable[
        sds_pb2.DataFrameUID,
        sds_pb2.DataFrameContents,
    ]
    """eagerly evaluate the dataframe, then return its contents"""

    Join: grpc.UnaryUnaryMultiCallable[
        sds_pb2.JoinRequest,
        sds_pb2.DataFrameUID,
    ]
    """Join 2 dataframes together into one"""

    Distinct: grpc.UnaryUnaryMultiCallable[
        sds_pb2.DataFrameUID,
        sds_pb2.DataFrameUID,
    ]
    """Return a new DataFrame whose contents are the same as the given
    DataFrame, except with duplicate rows removed
    """

    Union: grpc.UnaryUnaryMultiCallable[
        sds_pb2.UnionRequest,
        sds_pb2.DataFrameUID,
    ]
    """Combine two DataFrames together to calculate the union of the two.

    Both DataFrames must have the same schema. If they do not, return
    an error. If they do, preserve duplicate rows in the final result.
    """

    Drop: grpc.UnaryUnaryMultiCallable[
        sds_pb2.DropRequest,
        sds_pb2.DataFrameUID,
    ]
    """return a new DataFrame with the given columns missing.

    if you pass a column that does not exist, this entire operation
    will be a no-op and you'll get the same UUID back
    """

    ExportCSV: grpc.UnaryUnaryMultiCallable[
        sds_pb2.DataFrameUID,
        sds_pb2.CSVOutput,
    ]
    """export a CSV file and return it in the response"""

class sdsAsyncStub:
    """//////////////////////////////////////
    session/dataframe initialization methods
    //////////////////////////////////////
    """

    ExecuteSql: grpc.aio.UnaryUnaryMultiCallable[
        sds_pb2.ExecuteSqlRequest,
        sds_pb2.DataFrameUID,
    ]
    """execute SQL against a session and return a new DataFrame representing
    the result
    """

    CreateSession: grpc.aio.UnaryUnaryMultiCallable[
        sds_pb2.Empty,
        sds_pb2.SessionUID,
    ]
    """create a new session with no DataFrames therein, then return its UID"""

    LoadCSV: grpc.aio.UnaryUnaryMultiCallable[
        sds_pb2.LoadFileRequest,
        sds_pb2.DataFrameUID,
    ]
    """Load a CSV into a DataFrame, then return its UID"""

    LoadParquet: grpc.aio.UnaryUnaryMultiCallable[
        sds_pb2.LoadFileRequest,
        sds_pb2.DataFrameUID,
    ]
    """Load a Parquet file into a DataFrame, then return its UID"""

    LoadJSON: grpc.aio.UnaryUnaryMultiCallable[
        sds_pb2.LoadFileRequest,
        sds_pb2.DataFrameUID,
    ]
    """Load a JSON file into a DataFrame, then return its UID"""

    CloseSession: grpc.aio.UnaryUnaryMultiCallable[
        sds_pb2.SessionUID,
        sds_pb2.Empty,
    ]
    """//////////////////////////////////////
    session destruction methods
    //////////////////////////////////////

    Close a session and free all associated dataframes.

    This is a highly destructive method, because all dataframes
    created directly or indirectly as part of this session
    will be instantly deleted.
    """

    SaveDataFrameAsTable: grpc.aio.UnaryUnaryMultiCallable[
        sds_pb2.SaveDataFrameAsTableRequest,
        sds_pb2.Empty,
    ]
    """//////////////////////////////////////
    dataframe instance methods
    //////////////////////////////////////

    save a dataframe as a table, so that you can execute SQL queries 
    against it
    """

    PrettyPrintDataframe: grpc.aio.UnaryUnaryMultiCallable[
        sds_pb2.DataFrameUID,
        sds_pb2.PrettyPrintDataframeResponse,
    ]
    """pretty-print a given dataframe.

    TODO: throw an error if the dataframe is too big to pretty-print
    """

    LimitDataFrame: grpc.aio.UnaryUnaryMultiCallable[
        sds_pb2.LimitDataFrameRequest,
        sds_pb2.DataFrameUID,
    ]
    """filter an existing DataFrame, and return a new DataFrame"""

    SortDataFrame: grpc.aio.UnaryUnaryMultiCallable[
        sds_pb2.SortDataFrameRequest,
        sds_pb2.DataFrameUID,
    ]
    """sort a dataframe by 1 or more column(s)"""

    FilterDataFrame: grpc.aio.UnaryUnaryMultiCallable[
        sds_pb2.FilterDataFrameRequest,
        sds_pb2.DataFrameUID,
    ]
    """filter a dataframe according to 1 or more filter conditions"""

    Aggregate: grpc.aio.UnaryUnaryMultiCallable[
        sds_pb2.AggregateRequest,
        sds_pb2.DataFrameUID,
    ]
    """group by, then aggregate a dataframe's data, the return a new dataframe"""

    WithColumn: grpc.aio.UnaryUnaryMultiCallable[
        sds_pb2.WithColumnRequest,
        sds_pb2.DataFrameUID,
    ]
    """add a new column -- optionally by doing some calculation -- to a 
    given dataframe
    """

    Select: grpc.aio.UnaryUnaryMultiCallable[
        sds_pb2.SelectRequest,
        sds_pb2.DataFrameUID,
    ]
    """project a DataFrame onto a new one, optionally by calculating new
    values
    """

    Collect: grpc.aio.UnaryUnaryMultiCallable[
        sds_pb2.DataFrameUID,
        sds_pb2.DataFrameContents,
    ]
    """eagerly evaluate the dataframe, then return its contents"""

    Join: grpc.aio.UnaryUnaryMultiCallable[
        sds_pb2.JoinRequest,
        sds_pb2.DataFrameUID,
    ]
    """Join 2 dataframes together into one"""

    Distinct: grpc.aio.UnaryUnaryMultiCallable[
        sds_pb2.DataFrameUID,
        sds_pb2.DataFrameUID,
    ]
    """Return a new DataFrame whose contents are the same as the given
    DataFrame, except with duplicate rows removed
    """

    Union: grpc.aio.UnaryUnaryMultiCallable[
        sds_pb2.UnionRequest,
        sds_pb2.DataFrameUID,
    ]
    """Combine two DataFrames together to calculate the union of the two.

    Both DataFrames must have the same schema. If they do not, return
    an error. If they do, preserve duplicate rows in the final result.
    """

    Drop: grpc.aio.UnaryUnaryMultiCallable[
        sds_pb2.DropRequest,
        sds_pb2.DataFrameUID,
    ]
    """return a new DataFrame with the given columns missing.

    if you pass a column that does not exist, this entire operation
    will be a no-op and you'll get the same UUID back
    """

    ExportCSV: grpc.aio.UnaryUnaryMultiCallable[
        sds_pb2.DataFrameUID,
        sds_pb2.CSVOutput,
    ]
    """export a CSV file and return it in the response"""

class sdsServicer(metaclass=abc.ABCMeta):
    """//////////////////////////////////////
    session/dataframe initialization methods
    //////////////////////////////////////
    """

    @abc.abstractmethod
    def ExecuteSql(
        self,
        request: sds_pb2.ExecuteSqlRequest,
        context: _ServicerContext,
    ) -> typing.Union[
        sds_pb2.DataFrameUID, collections.abc.Awaitable[sds_pb2.DataFrameUID]
    ]:
        """execute SQL against a session and return a new DataFrame representing
        the result
        """

    @abc.abstractmethod
    def CreateSession(
        self,
        request: sds_pb2.Empty,
        context: _ServicerContext,
    ) -> typing.Union[
        sds_pb2.SessionUID, collections.abc.Awaitable[sds_pb2.SessionUID]
    ]:
        """create a new session with no DataFrames therein, then return its UID"""

    @abc.abstractmethod
    def LoadCSV(
        self,
        request: sds_pb2.LoadFileRequest,
        context: _ServicerContext,
    ) -> typing.Union[
        sds_pb2.DataFrameUID, collections.abc.Awaitable[sds_pb2.DataFrameUID]
    ]:
        """Load a CSV into a DataFrame, then return its UID"""

    @abc.abstractmethod
    def LoadParquet(
        self,
        request: sds_pb2.LoadFileRequest,
        context: _ServicerContext,
    ) -> typing.Union[
        sds_pb2.DataFrameUID, collections.abc.Awaitable[sds_pb2.DataFrameUID]
    ]:
        """Load a Parquet file into a DataFrame, then return its UID"""

    @abc.abstractmethod
    def LoadJSON(
        self,
        request: sds_pb2.LoadFileRequest,
        context: _ServicerContext,
    ) -> typing.Union[
        sds_pb2.DataFrameUID, collections.abc.Awaitable[sds_pb2.DataFrameUID]
    ]:
        """Load a JSON file into a DataFrame, then return its UID"""

    @abc.abstractmethod
    def CloseSession(
        self,
        request: sds_pb2.SessionUID,
        context: _ServicerContext,
    ) -> typing.Union[sds_pb2.Empty, collections.abc.Awaitable[sds_pb2.Empty]]:
        """//////////////////////////////////////
        session destruction methods
        //////////////////////////////////////

        Close a session and free all associated dataframes.

        This is a highly destructive method, because all dataframes
        created directly or indirectly as part of this session
        will be instantly deleted.
        """

    @abc.abstractmethod
    def SaveDataFrameAsTable(
        self,
        request: sds_pb2.SaveDataFrameAsTableRequest,
        context: _ServicerContext,
    ) -> typing.Union[sds_pb2.Empty, collections.abc.Awaitable[sds_pb2.Empty]]:
        """//////////////////////////////////////
        dataframe instance methods
        //////////////////////////////////////

        save a dataframe as a table, so that you can execute SQL queries
        against it
        """

    @abc.abstractmethod
    def PrettyPrintDataframe(
        self,
        request: sds_pb2.DataFrameUID,
        context: _ServicerContext,
    ) -> typing.Union[
        sds_pb2.PrettyPrintDataframeResponse,
        collections.abc.Awaitable[sds_pb2.PrettyPrintDataframeResponse],
    ]:
        """pretty-print a given dataframe.

        TODO: throw an error if the dataframe is too big to pretty-print
        """

    @abc.abstractmethod
    def LimitDataFrame(
        self,
        request: sds_pb2.LimitDataFrameRequest,
        context: _ServicerContext,
    ) -> typing.Union[
        sds_pb2.DataFrameUID, collections.abc.Awaitable[sds_pb2.DataFrameUID]
    ]:
        """filter an existing DataFrame, and return a new DataFrame"""

    @abc.abstractmethod
    def SortDataFrame(
        self,
        request: sds_pb2.SortDataFrameRequest,
        context: _ServicerContext,
    ) -> typing.Union[
        sds_pb2.DataFrameUID, collections.abc.Awaitable[sds_pb2.DataFrameUID]
    ]:
        """sort a dataframe by 1 or more column(s)"""

    @abc.abstractmethod
    def FilterDataFrame(
        self,
        request: sds_pb2.FilterDataFrameRequest,
        context: _ServicerContext,
    ) -> typing.Union[
        sds_pb2.DataFrameUID, collections.abc.Awaitable[sds_pb2.DataFrameUID]
    ]:
        """filter a dataframe according to 1 or more filter conditions"""

    @abc.abstractmethod
    def Aggregate(
        self,
        request: sds_pb2.AggregateRequest,
        context: _ServicerContext,
    ) -> typing.Union[
        sds_pb2.DataFrameUID, collections.abc.Awaitable[sds_pb2.DataFrameUID]
    ]:
        """group by, then aggregate a dataframe's data, the return a new dataframe"""

    @abc.abstractmethod
    def WithColumn(
        self,
        request: sds_pb2.WithColumnRequest,
        context: _ServicerContext,
    ) -> typing.Union[
        sds_pb2.DataFrameUID, collections.abc.Awaitable[sds_pb2.DataFrameUID]
    ]:
        """add a new column -- optionally by doing some calculation -- to a
        given dataframe
        """

    @abc.abstractmethod
    def Select(
        self,
        request: sds_pb2.SelectRequest,
        context: _ServicerContext,
    ) -> typing.Union[
        sds_pb2.DataFrameUID, collections.abc.Awaitable[sds_pb2.DataFrameUID]
    ]:
        """project a DataFrame onto a new one, optionally by calculating new
        values
        """

    @abc.abstractmethod
    def Collect(
        self,
        request: sds_pb2.DataFrameUID,
        context: _ServicerContext,
    ) -> typing.Union[
        sds_pb2.DataFrameContents, collections.abc.Awaitable[sds_pb2.DataFrameContents]
    ]:
        """eagerly evaluate the dataframe, then return its contents"""

    @abc.abstractmethod
    def Join(
        self,
        request: sds_pb2.JoinRequest,
        context: _ServicerContext,
    ) -> typing.Union[
        sds_pb2.DataFrameUID, collections.abc.Awaitable[sds_pb2.DataFrameUID]
    ]:
        """Join 2 dataframes together into one"""

    @abc.abstractmethod
    def Distinct(
        self,
        request: sds_pb2.DataFrameUID,
        context: _ServicerContext,
    ) -> typing.Union[
        sds_pb2.DataFrameUID, collections.abc.Awaitable[sds_pb2.DataFrameUID]
    ]:
        """Return a new DataFrame whose contents are the same as the given
        DataFrame, except with duplicate rows removed
        """

    @abc.abstractmethod
    def Union(
        self,
        request: sds_pb2.UnionRequest,
        context: _ServicerContext,
    ) -> typing.Union[
        sds_pb2.DataFrameUID, collections.abc.Awaitable[sds_pb2.DataFrameUID]
    ]:
        """Combine two DataFrames together to calculate the union of the two.

        Both DataFrames must have the same schema. If they do not, return
        an error. If they do, preserve duplicate rows in the final result.
        """

    @abc.abstractmethod
    def Drop(
        self,
        request: sds_pb2.DropRequest,
        context: _ServicerContext,
    ) -> typing.Union[
        sds_pb2.DataFrameUID, collections.abc.Awaitable[sds_pb2.DataFrameUID]
    ]:
        """return a new DataFrame with the given columns missing.

        if you pass a column that does not exist, this entire operation
        will be a no-op and you'll get the same UUID back
        """

    @abc.abstractmethod
    def ExportCSV(
        self,
        request: sds_pb2.DataFrameUID,
        context: _ServicerContext,
    ) -> typing.Union[sds_pb2.CSVOutput, collections.abc.Awaitable[sds_pb2.CSVOutput]]:
        """export a CSV file and return it in the response"""

def add_sdsServicer_to_server(
    servicer: sdsServicer, server: typing.Union[grpc.Server, grpc.aio.Server]
) -> None: ...
